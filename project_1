## Saya akan buat video tutorialnya di Youtube. Untuk saat ini, silakan kunjungi dan subscribe channel saya -> https://www.youtube.com/channel/UCEf5r6lPK1oImal2gxZJllw
## I will make tutorial videos on Youtube. In the meantime, please visit and subscribe my channel -> https://www.youtube.com/channel/UCEf5r6lPK1oImal2gxZJllw


# Persiapan awal / Setting up things

download.file(url = "http://curl.haxx.se/ca/cacert.pem", destfile = "cacert.pem")

library(twitteR)

setup_twitter_oauth("aaaa",
                    "bbbb",
                    "cccc",
                    "dddd")
##################################################################################

# Akuisisi tweets / acquire tweets

tweets <- searchTwitter("kueri / query", n = 2000, lang = "id")

df_tweets <- twListToDF(sentimen)
#################################################################################

# Buat file .csv / create .csv file

write.csv(df_tweets$text, file = "file_name.csv", row.names = F)
##################################################################################

# Pre-processing di RStudio / Pre-processing at RStudio

teks_sentimen <- read.csv("lbl_teks_sentimen.csv", sep = ";")
teks_sentimen$teks <- as.character(teks_sentimen$teks)
teks_sentimen$teks <- tolower(teks_sentimen$teks)
teks_sentimen$teks <- gsub("https://t.co/\\w+", " ", teks_sentimen$teks)
teks_sentimen$teks <- gsub("@\\w+", " ", teks_sentimen$teks)
teks_sentimen$teks <- gsub("[[:punct:]]|.", " ", teks_sentimen$teks)
teks_sentimen$teks <- gsub("[[:digit:]]", " ", teks_sentimen$teks)

library(tm)
stopword_bindo <- read.csv("id.stopwords.02.01.2016.csv", header = FALSE)
stopword_bindo <- as.character(stopword_bindo$V1)
stopword_bindo <- c(stopword_bindo, stopwords())
teks_sentimen$teks <- removeWords(teks_sentimen$teks, stopword_bindo)
teks_sentimen$teks <- stripWhitespace(teks_sentimen$teks)
##################################################################################

# Vektorisasi / Vectorization

dtm_sentimen <- DocumentTermMatrix(
                    Vcorpus(
                        VectorSource(teks_sentimen$teks)
                    )
                )
##################################################################################

# Buat set data training dan testing / Create training and testing dataset

dtm_sentimen_latih <- dtm_sentimen[1:1800, ]
dtm_sentimen_uji <- dtm_sentimen[1801:2000, ]

label_sentimen_latih <- teks_sentimen[1:1800, ]$label
label_sentimen_uji <- teks_sentimen[1801:2000,]$label

konversi_jumlah <- function(x){
                       x <- ifelse(x > 0, "Ada",
                      "TAda")
                    }

sentimen_latih <- apply(dtm_sentimen_latih, MARGIN = 2, konversi_jumlah)
sentimen_uji <- apply(dtm_sentimen_uji, MARGIN = 2, konversi_jumlah)
##################################################################################

# Tahap training / Training step

library(e1071)

pengklas_sentimen <- naiveBayes(sentimen_latih,label_sentimen_latih)
prediksi_sentimen <- predict(pengklas_sentimen,sentimen_uji) 
##################################################################################

# Buat tabulasi silang / Create cross-table

library(gmodels)

CrossTable(prediksi_sentimen, label_sentimen_uji,
           prop.chisq = F, prop.t = F,
           dnn = c('terprediksi', 'manual'))
##################################################################################

# Visualisasi / Visualization

pie(prop.table(table(prediksi_sentimen)),
    labels = paste(pengklas_sentimen$levels,
                   "\n", table(prediksi_sentimen), sep = " "),
    col = rainbow(4), main = "Diagram Hasil Analisis
                              \n (Berdasarkan Set Data Testing
                              \n Sebanyak 200 Teks Tweets)")
